# ParCo: Part-Coordinating Text-to-Motion Synthesis

<p align="center">
  <img src="../docs/imgs/teaser1.png" width="30%" />
</p>

Pytorchå®ç°è®ºæ–‡ï¼š [ParCo: Part-Coordinating Text-to-Motion Synthesis](https://arxiv.org/abs/2403.18512)ã€‚

[![Paper](http://img.shields.io/badge/Paper-arxiv.2403.18512-FF6B6B.svg)](https://arxiv.org/abs/2403.18512)
[![Language](http://img.shields.io/badge/Language-English-D6EAF8.svg)](../README.md)
[![Colab](http://img.shields.io/badge/Demo-Open%20in%20Colab-FFD93D.svg)](https://colab.research.google.com/drive/1mGYpqIoB7BWgvfm7xxTZ4bUYPaeBRn2D?usp=sharing)

<p align="center">
<table>

  <tr>
    <th colspan="4">æ–‡æœ¬: "a person is having a hearty laugh and makes a jovial motion with their left hand."</th>
  </tr>
  <tr>
    <th>ğŸ”¥ParCo (Ours)ğŸ”¥</th>
    <th><u><a href="https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html"><nobr>ReMoDiffuse</nobr> </a></u></th>
    <th><u><a href="https://mael-zys.github.io/T2M-GPT/"><nobr>T2M-GPT</nobr> </a></u></th>
    <th><u><a href="https://mingyuan-zhang.github.io/projects/MotionDiffuse.html"><nobr>MotionDiffuse</nobr> </a></u></th>
  </tr>
  <tr>
    <td><img src="../docs/imgs/parco/parco_5.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/remodiffuse/remodiff_5.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/t2mgpt/t2mgpt_5.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/motiondiffuse/motiondiffuse_5.gif" width="160px" alt="gif"></td>
  </tr>


  <tr>
    <th colspan="4">æ–‡æœ¬: "standing on one leg and hopping."</th>
  </tr>
  <tr>
    <th>ğŸ”¥ParCo (Ours)ğŸ”¥</th>
    <th><u><a href="https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html"><nobr>ReMoDiffuse</nobr> </a></u></th>
    <th><u><a href="https://mael-zys.github.io/T2M-GPT/"><nobr>T2M-GPT</nobr> </a></u></th>
    <th><u><a href="https://mingyuan-zhang.github.io/projects/MotionDiffuse.html"><nobr>MotionDiffuse</nobr> </a></u></th>
  </tr>
  <tr>
    <td><img src="../docs/imgs/parco/parco_9.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/remodiffuse/remodiff_9.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/t2mgpt/t2mgpt_9.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/motiondiffuse/motiondiffuse_9.gif" width="160px" alt="gif"></td>
  </tr>


  <tr>
    <th colspan="4">æ–‡æœ¬: "a man steps back, picks something up and put it to his head and then puts it back."</th>
  </tr>
  <tr>
    <th>ğŸ”¥ParCo (Ours)ğŸ”¥</th>
    <th><u><a href="https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html"><nobr>ReMoDiffuse</nobr> </a></u></th>
    <th><u><a href="https://mael-zys.github.io/T2M-GPT/"><nobr>T2M-GPT</nobr> </a></u></th>
    <th><u><a href="https://mingyuan-zhang.github.io/projects/MotionDiffuse.html"><nobr>MotionDiffuse</nobr> </a></u></th>
  </tr>
  <tr>
    <td><img src="../docs/imgs/parco/parco_2.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/remodiffuse/remodiff_2.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/t2mgpt/t2mgpt_2.gif" width="160px" alt="gif"></td>
    <td><img src="../docs/imgs/motiondiffuse/motiondiffuse_2.gif" width="160px" alt="gif"></td>
  </tr>


</table>
</p>


å¦‚æœæˆ‘ä»¬çš„é¡¹ç›®å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ç»™è¿™ä¸ªä»“åº“**åŠ æ˜Ÿ**å¹¶**å¼•ç”¨**æˆ‘ä»¬çš„è®ºæ–‡ï¼š
```
@article{zou2024parco,
  title={ParCo: Part-Coordinating Text-to-Motion Synthesis},
  author={Zou, Qiran and Yuan, Shangyuan and Du, Shian and Wang, Yu and Liu, Chang and Xu, Yi and Chen, Jie and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2403.18512},
  year={2024}
}
```

## è®¡ç®—èµ„æºæ¶ˆè€—

### è®­ç»ƒ
è®­ç»ƒæ‰€æ¶ˆè€—çš„æ—¶é—´å’ŒGPUå†…å­˜ï¼ˆå•ä¸ªA100 GPUï¼‰ï¼š

|        | (é˜¶æ®µä¸€) VQ-VAE | (é˜¶æ®µäºŒ) Part-Coordinated Transformer |
|:-------|:----------------:|:--------------------------------------:|
| æ—¶é—´   |      20.5h       |                 52.3h                  |
| å†…å­˜   |      3.5GB       |                 28.4GB                 |


### æ¨ç†


| æ–¹æ³•       | Param(M) | FLOPs(G) | InferTime(s) |
|:-------------|:--------:|:--------:|:------------:|
| ReMoDiffuse  |  198.2   |  481.0   |    0.091     |
| T2M-GPT      |  237.6   |  292.3   |    0.544     |
| ParCo (Ours) |  168.4   |  211.7   |    0.036     |

- å¯¹äºFLOPsæŒ‡æ ‡ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹ç”Ÿæˆä¸€ä¸ªåŒ…å«200å¸§çš„å•ä¸ªåŠ¨ä½œæ ·æœ¬æ—¶è®¡ç®—FLOPsã€‚
- æˆ‘ä»¬æŠ¥å‘Šäº†ç”Ÿæˆå•ä¸ªæ ·æœ¬æ‰€æ¶ˆè€—çš„æ—¶é—´ï¼Œä½œä¸ºInferTimeæŒ‡æ ‡ã€‚
  æˆ‘ä»¬ç”Ÿæˆäº†10,000ä¸ªæ ·æœ¬ï¼Œå¹¶è®¡ç®—äº†æ¯ä¸ªæ ·æœ¬çš„å¹³å‡æ¨ç†æ—¶é—´ã€‚
  å¯¹äºReMoDiffå’Œæˆ‘ä»¬çš„ParCoï¼Œæˆ‘ä»¬å°†æ‰¹å¤„ç†å¤§å°è®¾ç½®ä¸º100ï¼ˆT2M-GPTä¸æ”¯æŒæ‰¹å¤„ç†å¹¶è¡Œæ¨ç†ï¼‰ã€‚


## ç›®å½•

- [1. å¿«é€Ÿå¯åŠ¨Demo](#1-å¿«é€Ÿå¯åŠ¨Demo)
- [2. å®‰è£…](#2-å®‰è£…)
- [3. è®­ç»ƒ](#3-è®­ç»ƒ)
- [4. è¯„ä¼°](#4-è¯„ä¼°)
- [5. é¢„è®­ç»ƒæ¨¡å‹](#5-é¢„è®­ç»ƒæ¨¡å‹)
- [6. ä¸ŠåŠèº«å’Œä¸‹åŠèº«åˆ†åŒº](#6-ä¸ŠåŠèº«å’Œä¸‹åŠèº«åˆ†åŒº)
- [7. å¯è§†åŒ–åŠ¨ä½œ](#7-å¯è§†åŒ–åŠ¨ä½œ)
- [TODO](#todo)
- [è‡´è°¢](#è‡´è°¢)



## 1. å¿«é€Ÿå¯åŠ¨Demo

### 1.1. Colab Demo

ğŸ‘‰ å°è¯•æˆ‘ä»¬çš„[Colab demo](https://colab.research.google.com/drive/1mGYpqIoB7BWgvfm7xxTZ4bUYPaeBRn2D?usp=sharing) !

æˆ‘ä»¬çš„æ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•å‡†å¤‡ç¯å¢ƒä»¥åŠå¦‚ä½•ä½¿ç”¨ParCoè¿›è¡Œæ¨ç†ã€‚
ä½ ä¹Ÿå¯ä»¥æ–¹ä¾¿åœ°æ¢ç´¢æˆ‘ä»¬çš„ParCoã€‚

å¦‚æœä½ å¸Œæœ›é‡ç°ParCoçš„å¯è§†åŒ–ç»“æœï¼Œæˆ‘ä»¬å»ºè®®æ ¹æ®æˆ‘ä»¬çš„æ•™ç¨‹åœ¨æœ¬åœ°å®‰è£…ç¯å¢ƒå¹¶åœ¨é‚£é‡Œè¿›è¡Œé‡ç°ï¼ˆå› ä¸ºColabå’Œæœ¬åœ°è¿è¡Œçš„ç»“æœæœ‰æ‰€ä¸åŒï¼‰ã€‚
è¿™å¯èƒ½æ˜¯ç”±äºColabå’Œæœ¬åœ°è®­ç»ƒ/æµ‹è¯•ä¹‹é—´GPUå’ŒCUDAç¯å¢ƒçš„å·®å¼‚æ‰€å¯¼è‡´çš„ã€‚

<p align="center">
<img src="../docs/imgs/demo_screenshot.png" width="40%" />
</p>

### 1.2. æœ¬åœ°å¿«é€Ÿæ¨ç†

å®‰è£…å®Œæˆåï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ä½ è‡ªå·±çš„æ–‡æœ¬è¾“å…¥ç”ŸæˆåŠ¨ä½œï¼ˆ.gifæ ¼å¼ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```
CUDA_VISIBLE_DEVICES=0 python visualize/infer_motion_npy.py \
--eval-exp-dir output/00001-t2m/VQVAE-ParCo-t2m-default/00000-Trans-ParCo-default \
--select-ckpt fid \
--infer-mode userinput \
--input-text 'an idol trainee is dancing like a basketball dribbling.' \
--skip-path-check
```
ç”Ÿæˆçš„åŠ¨ä½œå¯è§†åŒ–æ ·æœ¬è¢«ä¿å­˜ä¸º`output/visualize/XXXXX-userinput/skeleton_viz.gif`ã€‚

<p align="center">
<img src="../docs/imgs/demo_local_infer.gif" width="30%" />
</p>

## 2. å®‰è£…

### 2.1. ç¯å¢ƒ

æˆ‘ä»¬çš„æ¨¡å‹åœ¨å•ä¸ªA100-40G GPUä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œæµ‹è¯•ï¼Œè½¯ä»¶ç¯å¢ƒä¸ºï¼šPython 3.7.11ã€PyTorch 1.10.1ã€CUDA 11.3.1ã€cuDNN 8.2.0ã€Ubuntu 20.04ã€‚

- CUDA & cuDNN (cuDNNå¯èƒ½ä¸æ˜¯å¿…éœ€çš„)

    CUDAå’ŒcuDNNåº”è¯¥é¦–å…ˆå®‰è£…ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ç‰ˆæœ¬ï¼š
    `CUDA: 11.3.1`å’Œ`cuDNN: 8.2.0`ã€‚

  - å®‰è£…CUDA 11.3:
    1. ä»[è¿™é‡Œ](https://developer.nvidia.com/cuda-11-3-1-download-archive)ä¸‹è½½
       (æˆ‘ä»¬å»ºè®®é€‰æ‹©å®‰è£…å™¨ç±»å‹ä¸º`runfile (local)`)ã€‚
    1. ä½¿ç”¨å®˜æ–¹ç½‘ç«™æä¾›çš„è„šæœ¬å®‰è£…runfileã€‚
    1. åœ¨è¿è¡Œæˆ‘ä»¬çš„ä»£ç ä¹‹å‰ï¼Œæ£€æŸ¥ä½ çš„CUDAç¯å¢ƒæ˜¯å¦å·²é“¾æ¥åˆ°CUDA 11.3(è¿è¡Œ`nvcc --version`æ¥æ£€æŸ¥)ã€‚
    å¦‚æœç‰ˆæœ¬ä¸æ˜¯11.3ï¼Œä½ éœ€è¦é€šè¿‡`export PATH=/usr/local/cuda-11.3/bin:$PATH`å°†CUDAè·¯å¾„æ·»åŠ åˆ°ä½ çš„ç¯å¢ƒä¸­ï¼Œç„¶åå†æ¬¡æ£€æŸ¥ç‰ˆæœ¬ã€‚
    
  - å®‰è£…cuDNN 8.2.0:
    1. ä»[è¿™é‡Œ](https://developer.nvidia.com/rdp/cudnn-archive)ä¸‹è½½(æ ¹æ®ä½ çš„ç³»ç»Ÿé€‰æ‹©`8.2.0 for CUDA 11.X`å’Œ`cuDNN Library for Linux/Windows`)
    1. æ ¹æ®[æŒ‡å¯¼](https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-820/install-guide/index.html#installlinux-tar)ä¸‹è½½cuDNNã€‚
    è¯·å‚è€ƒç¬¬`2.3. æ•°æ®é›†`ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ä¹‹å‰çš„é˜¶æ®µå·²ç»ä¸‹è½½äº†taræ–‡ä»¶ã€‚


- Condaç¯å¢ƒ
  
  ä½ éœ€è¦éµå¾ªä¸‹é¢çš„è„šæœ¬æ¥é¿å…æ½œåœ¨çš„åŒ…å†²çªã€‚å¦åˆ™ï¼Œå¯èƒ½æ— æ³•æ­£ç¡®å®‰è£…PyTorchæˆ–å®‰è£…æŸäº›åŒ…æ—¶å¤±è´¥ã€‚

  - åˆ›å»ºcondaç¯å¢ƒ
    ```
    conda create -n ParCo blas=1.0 bzip2=1.0.8 ca-certificates=2021.7.5 certifi=2021.5.30 freetype=2.10.4 gmp=6.2.1 gnutls=3.6.15 intel-openmp=2021.3.0 jpeg=9b lame=3.100 lcms2=2.12 ld_impl_linux-64=2.35.1 libffi=3.3 libgcc-ng=9.3.0 libgomp=9.3.0 libiconv=1.15 libidn2=2.3.2 libpng=1.6.37 libstdcxx-ng=9.3.0 libtasn1=4.16.0 libtiff=4.2.0 libunistring=0.9.10 libuv=1.40.0 libwebp-base=1.2.0 lz4-c=1.9.3 mkl=2021.3.0 mkl-service=2.4.0 mkl_fft=1.3.0 mkl_random=1.2.2 ncurses=6.2 nettle=3.7.3 ninja=1.10.2 numpy=1.20.3 numpy-base=1.20.3 olefile=0.46 openh264=2.1.0 openjpeg=2.3.0 openssl=1.1.1k pillow=8.3.1 pip=21.0.1 readline=8.1 setuptools=52.0.0 six=1.16.0 sqlite=3.36.0 tk=8.6.10 typing_extensions=3.10.0.0 wheel=0.37.0 xz=5.2.5 zlib=1.2.11 zstd=1.4.9 python=3.7
    ```
    ```
    conda activate ParCo
    ```
  - å®‰è£…å¿…è¦çš„åŒ…(æ‰§è¡Œä»¥ä¸‹æ‰€æœ‰è„šæœ¬)
    ```
    conda install ffmpeg=4.3 -c pytorch
    conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=11.3 -c pytorch -c conda-forge
    ```
    ``` 
    pip install absl-py==0.13.0 backcall==0.2.0 cachetools==4.2.2 charset-normalizer==2.0.4 chumpy==0.70 cycler==0.10.0 decorator==5.0.9 google-auth==1.35.0 google-auth-oauthlib==0.4.5 grpcio==1.39.0 idna==3.2 imageio==2.9.0 ipdb==0.13.9 ipython==7.26.0 ipython-genutils==0.2.0 jedi==0.18.0 joblib==1.0.1 kiwisolver==1.3.1 markdown==3.3.4 matplotlib==3.4.3 matplotlib-inline==0.1.2 oauthlib==3.1.1 pandas==1.3.2 parso==0.8.2 pexpect==4.8.0 pickleshare==0.7.5 prompt-toolkit==3.0.20 protobuf==3.17.3 ptyprocess==0.7.0 pyasn1==0.4.8 pyasn1-modules==0.2.8 pygments==2.10.0 pyparsing==2.4.7 python-dateutil==2.8.2 pytz==2021.1 pyyaml==5.4.1 requests==2.26.0 requests-oauthlib==1.3.0 rsa==4.7.2 scikit-learn==0.24.2 scipy==1.7.1 sklearn==0.0 smplx==0.1.28 tensorboard==2.6.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.0 threadpoolctl==2.2.0 toml==0.10.2 tqdm==4.62.2 traitlets==5.0.5 urllib3==1.26.6 wcwidth==0.2.5 werkzeug==2.0.1 git+https://github.com/openai/CLIP.git git+https://github.com/nghorbani/human_body_prior gdown moviepy
    ```
    ```
    pip install imageio-ffmpeg
    pip install importlib-metadata==4.13.0
    ```

  - å®‰è£…æ¸²æŸ“åŠ¨ä½œæ‰€éœ€çš„åŒ…ï¼ˆå¯é€‰ï¼‰
    ```
    bash dataset/prepare/download_smpl.sh
    conda install -c menpo osmesa
    conda install h5py
    conda install -c conda-forge shapely pyrender trimesh==3.22.5 mapbox_earcut
    ```
    å¦‚æœä½ åœ¨ä½¿ç”¨ä»£ç†è®¿é—®Google Driveï¼Œè¯·ä½¿ç”¨`bash dataset/prepare/use_proxy/download_smpl.sh`è¿›è¡Œä¸‹è½½ã€‚
    è„šæœ¬ä¸­é»˜è®¤çš„ä»£ç†ç«¯å£è®¾ç½®ä¸º`1087`ï¼Œä½ å¯ä»¥å°†å…¶ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„ä»£ç†ç«¯å£ã€‚

### 2.2. ç‰¹å¾æå–å™¨

æˆ‘ä»¬ä½¿ç”¨[T2M](https://github.com/EricGuo5513/text-to-motion)æä¾›çš„æå–å™¨è¿›è¡Œè¯„ä¼°ã€‚
è¯·ä¸‹è½½æå–å™¨å’Œgloveè¯å‘é‡å™¨ã€‚æ³¨æ„ï¼Œç³»ç»Ÿä¸­åº”é¢„å…ˆå®‰è£…äº†'zip'ï¼Œå¦‚æœæ²¡æœ‰ï¼Œè¯·è¿è¡Œ`sudo apt-get install zip`æ¥å®‰è£…zipã€‚

```
bash dataset/prepare/download_glove.sh
bash dataset/prepare/download_extractor.sh
```
å¦‚æœä½ åœ¨ä½¿ç”¨ä»£ç†è®¿é—®Google Driveï¼Œè¯·ä½¿ç”¨ä¸‹é¢çš„è„šæœ¬è¿›è¡Œä¸‹è½½ã€‚è„šæœ¬ä¸­é»˜è®¤çš„ä»£ç†ç«¯å£è®¾ç½®ä¸º`1087`ï¼Œä½ å¯ä»¥ä¿®æ”¹è„šæœ¬ä»¥è®¾ç½®ä½ è‡ªå·±çš„ä»£ç†ç«¯å£ã€‚
```
bash dataset/prepare/use_proxy/download_glove.sh
bash dataset/prepare/use_proxy/download_extractor.sh
```

### 2.3. æ•°æ®é›†

æˆ‘ä»¬çš„é¡¹ç›®ä½¿ç”¨äº†ä¸¤ä¸ª3Däººä½“åŠ¨ä½œè¯­è¨€æ•°æ®é›†ï¼Œ[HumanML3D](https://github.com/EricGuo5513/HumanML3D)å’Œ[KIT-ML](https://arxiv.org/pdf/1607.03827.pdf)ã€‚
ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/EricGuo5513/HumanML3D).æ‰¾åˆ°è¿™ä¸¤ä¸ªæ•°æ®é›†çš„å‡†å¤‡å’Œè·å–ä¿¡æ¯ã€‚

ä½ ä¹Ÿå¯ä»¥ç›´æ¥ä¸‹è½½æˆ‘ä»¬å¤„ç†è¿‡çš„è¿™äº›æ•°æ®é›†ï¼š[[Google Drive]](https://drive.google.com/drive/folders/1BuxQWAWtxwauD7AqF0TIpWjoqujYKq8v?usp=share_link).

æ–‡ä»¶ç›®å½•åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š
```
./dataset/HumanML3D/
â”œâ”€â”€ new_joint_vecs/
â”œâ”€â”€ texts/
â”œâ”€â”€ Mean.npy # ä¸[HumanML3D](https://github.com/EricGuo5513/HumanML3D)ä¸­ç›¸åŒ
â”œâ”€â”€ Std.npy # ä¸[HumanML3D](https://github.com/EricGuo5513/HumanML3D)ä¸­ç›¸åŒ
â”œâ”€â”€ train.txt
â”œâ”€â”€ val.txt
â”œâ”€â”€ test.txt
â”œâ”€â”€ train_val.txt
â””â”€â”€ all.txt
```

## 3. è®­ç»ƒ

æˆ‘ä»¬é¡¹ç›®çš„å®éªŒç›®å½•ç»“æ„æ˜¯ï¼š
```
./output  (arg.out_dir)
 â”œâ”€â”€ 00000-DATASET  (exp_number + dataset_name)
 â”‚   â””â”€â”€ VQVAE-EXP_NAME-DESC  (VQVAE + args.exp_name + desc)
 â”‚       â”œâ”€â”€ events.out.XXX
 â”‚       â”œâ”€â”€ net_best_XXX.pth
 â”‚       ...
 â”‚       â”œâ”€â”€ run.log
 â”‚       â”œâ”€â”€ test_vqvae
 â”‚       â”‚   â”œâ”€â”€ ...
 â”‚       â”‚   ...
 â”‚       â”œâ”€â”€ 0000-Trans-EXP_NAME-DESC  (stage2_exp_number + Trans + args.exp_name + desc)
 â”‚       â”‚   â”œâ”€â”€ quantized_dataset  (ä½¿ç”¨VQVAEé‡åŒ–çš„åŠ¨ä½œ)
 â”‚       â”‚   â”œâ”€â”€ events.out.XXX
 â”‚       â”‚   â”œâ”€â”€ net_best_XXX.pth
 â”‚       â”‚   ...
 â”‚       â”‚   â”œâ”€â”€ run.log
 â”‚       â”‚   â””â”€â”€ test_trans
 â”‚       â”‚       â”œâ”€â”€ ...
 â”‚       â”‚       ...
 â”‚       â”œâ”€â”€ 0001-Trans-EXP_NAME-DESC
 â”‚       ...
 â”œâ”€â”€ 00001-DATASET  (exp_number + dataset_name)
 ...
```


### 3.1. VQ-VAE

å¯¹äºKITæ•°æ®é›†ï¼Œè®¾ç½®`--dataname kit`.

```bash
CUDA_VISIBLE_DEVICES=0 python train_ParCo_vq.py \
--out-dir output \
--exp-name ParCo \
--dataname t2m \
--batch-size 256 \
--lr 2e-4 \
--total-iter 300000 \
--lr-scheduler 200000 \
--vqvae-cfg default \
--down-t 2 \
--depth 3 \
--dilation-growth-rate 3 \
--vq-act relu \
--quantizer ema_reset \
--loss-vel 0.5 \
--recons-loss l1_smooth
```

### 3.2. Part-Coordinated Transformer

è®°å¾—å°†`--vqvae-train-dir`è®¾ç½®ä¸ºä½ è®­ç»ƒçš„VQ-VAEå¯¹åº”çš„ç›®å½•ã€‚

å¯¹äºKITæ•°æ®é›†ï¼Œè®¾ç½®`--dataname kit`.

```bash
CUDA_VISIBLE_DEVICES=0 python train_ParCo_trans.py \
--vqvae-train-dir output/00000-t2m-ParCo/VQVAE-ParCo-t2m-default/ \
--select-vqvae-ckpt last \
--exp-name ParCo \
--pkeep 0.4 \
--batch-size 128 \
--trans-cfg default \
--fuse-ver V1_3 \
--alpha 1.0 \
--num-layers 14 \
--embed-dim-gpt 1024 \
--nb-code 512 \
--n-head-gpt 16 \
--block-size 51 \
--ff-rate 4 \
--drop-out-rate 0.1 \
--total-iter 300000 \
--eval-iter 10000 \
--lr-scheduler 150000 \
--lr 0.0001 \
--dataname t2m \
--down-t 2 \
--depth 3 \
--quantizer ema_reset \
--dilation-growth-rate 3 \
--vq-act relu
```


## 4. è¯„ä¼°
### 4.1. VQ-VAE

è®°å¾—å°†`--vqvae-train-dir`è®¾å®šä¸ºä½ æƒ³è¦è¯„ä¼°çš„VQ-VAE.
```bash
CUDA_VISIBLE_DEVICES=0 python eval_ParCo_vq.py --vqvae-train-dir output/00000-t2m-ParCo/VQVAE-ParCo-t2m-default/ --select-vqvae-ckpt last
```

### 4.2. Part-Coordinated Transformer

å¯¹äºåœ¨KIT-MLæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œè¯·è®¾ç½®`--select-ckpt last`.
å¦‚æœä½ æƒ³è¯„ä¼°å¤šæ¨¡æ€ï¼ˆéœ€è¦å¾ˆé•¿æ—¶é—´ï¼‰ï¼Œåªéœ€åˆ é™¤`--skip-mmod`.

è®°å¾—å°†`--eval-exp-dir` è®¾ç½®ä¸ºä½ è®­ç»ƒçš„ParCoçš„ç›®å½•ã€‚
```bash
CUDA_VISIBLE_DEVICES=0 python eval_ParCo_trans.py \
--eval-exp-dir output/00000-t2m-ParCo/VQVAE-ParCo-t2m-default/00000-Trans-ParCo-default \
--select-ckpt fid \
--skip-mmod
```



## 5. é¢„è®­ç»ƒæ¨¡å‹


æˆ‘ä»¬æä¾›äº†é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥åœ¨[[Google Drive]](https://drive.google.com/drive/folders/1MNiA76kChAPVZyhiwHFpOcoqz1mcZIhL?usp=share_link)ä¸­æ‰¾åˆ°ã€‚
è§£å‹ç¼©.zipæ–‡ä»¶å¹¶å°†å®ƒä»¬æ”¾åœ¨`output`æ–‡ä»¶å¤¹ä¸‹ä»¥ä¾›è¯„ä¼°ã€‚

ç”±äºæˆ‘ä»¬é‡å‘½åäº†é¢„è®­ç»ƒæ¨¡å‹çš„ç›®å½•ï¼Œè¯·åœ¨è¯„ä¼°æˆ‘ä»¬çš„Part-Coordinated transformeræ—¶è®°å¾—è®¾ç½®`--skip-path-check`ã€‚ä¾‹å¦‚ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 python eval_ParCo_trans.py \
--eval-exp-dir output/ParCo_official_HumanML3D/VQVAE-ParCo-t2m-default/00000-Trans-ParCo-default \
--select-ckpt fid \
--skip-mmod \
--skip-path-check
```

## 6. ä¸ŠåŠèº«å’Œä¸‹åŠèº«åˆ†åŒº
æˆ‘ä»¬çš„ParCoé‡‡ç”¨6éƒ¨åˆ†åˆ†åŒºç­–ç•¥ã€‚å¦‚æœæ‚¨æƒ³ç ”ç©¶ä¸ŠåŠèº«å’Œä¸‹åŠèº«åˆ†åŒºçš„ParCoï¼Œè¯·è¿è¡Œä¸‹é¢çš„è„šæœ¬ã€‚
<details>
<summary>
è¯¦æƒ…
</summary>

- è®­ç»ƒ(å¯¹äºKITæ•°æ®é›†ï¼Œè®¾ç½®`--dataname kit`):

  - VQ-VAE (ä¸Š&ä¸‹)
    ```
    CUDA_VISIBLE_DEVICES=0 python utils/ParCo_uplow/train_ParCo_vq_uplow.py \
    --out-dir output \
    --exp-name ParCo \
    --dataname t2m \
    --batch-size 256 \
    --lr 2e-4 \
    --total-iter 300000 \
    --lr-scheduler 200000 \
    --vqvae-cfg default \
    --down-t 2 \
    --depth 3 \
    --dilation-growth-rate 3 \
    --vq-act relu \
    --quantizer ema_reset \
    --loss-vel 0.5 \
    --recons-loss l1_smooth
    ```
    
  - Part-Coordinated Transformer (ä¸Š&ä¸‹)

    è®°å¾—å°†`--vqvae-train-dir`è®¾ç½®ä¸ºä½ è®­ç»ƒçš„VQ-VAEå¯¹åº”çš„ç›®å½•ã€‚ 

    ```
    CUDA_VISIBLE_DEVICES=0 python utils/ParCo_uplow/train_ParCo_trans_uplow.py \
    --vqvae-train-dir output/00000-t2m-ParCo-UpLow/VQVAE-ParCo-t2m-default/ \
    --select-vqvae-ckpt last \
    --exp-name ParCo \
    --pkeep 0.4 \
    --batch-size 128 \
    --trans-cfg default \
    --fuse-ver V1_3 \
    --alpha 1.0 \
    --num-layers 14 \
    --embed-dim-gpt 1024 \
    --nb-code 512 \
    --n-head-gpt 16 \
    --block-size 51 \
    --ff-rate 4 \
    --drop-out-rate 0.1 \
    --total-iter 300000 \
    --eval-iter 10000 \
    --lr-scheduler 150000 \
    --lr 0.0001 \
    --dataname t2m \
    --down-t 2 \
    --depth 3 \
    --quantizer ema_reset \
    --dilation-growth-rate 3 \
    --vq-act relu
    ```
    
- è¯„ä¼°:

  ç±»ä¼¼äº6éƒ¨åˆ†çš„è„šæœ¬ï¼Œåªéœ€å°†è¿è¡Œè„šæœ¬æ›´æ”¹ä¸º`utils/ParCo_uplow/`ç›®å½•ä¸‹çš„è„šæœ¬ã€‚ä¾‹å¦‚ï¼Œè¦è¯„ä¼°Part-Coordinated Transformerï¼ˆä¸Š&ä¸‹ï¼‰ï¼Œè„šæœ¬åº”è¯¥æ˜¯:

  ```
  CUDA_VISIBLE_DEVICES=0 python utils/ParCo_uplow/eval_ParCo_trans_uplow.py \
  --eval-exp-dir output/00000-t2m-ParCo-UpLow/VQVAE-ParCo-t2m-default/00000-Trans-ParCo-default \
  --select-ckpt fid \
  --skip-mmod
  ```
  è®°å¾—è®¾ç½®`--eval-exp-dir`åˆ°è®­ç»ƒæ¨¡å‹çš„ç›®å½•ã€‚
  
</details>



## 7. å¯è§†åŒ–åŠ¨ä½œ

æ¸²æŸ“SMPLç½‘æ ¼:

- é€‰é¡¹1ï¼šä½¿ç”¨æˆ‘ä»¬æä¾›çš„å·¥å…·ï¼ˆéƒ¨åˆ†æ¥æºäº[T2M-GPT](https://github.com/Mael-zys/T2M-GPT)ï¼‰:
  1. ç”Ÿæˆä»¥`.npy`æ ¼å¼ä¿å­˜çš„åŠ¨ä½œ:
  
     ç”Ÿæˆçš„ç»“æœå°†è¢«ä¿å­˜åœ¨`output/visualize/`ã€‚
     
     é€‰æ‹©æ‚¨å–œæ¬¢çš„æ¨¡å¼:
     - ç”±ç”¨æˆ·è¾“å…¥æ–‡æœ¬:
       ```
       CUDA_VISIBLE_DEVICES=0 python visualize/infer_motion_npy.py \
       --eval-exp-dir output/ParCo_official_HumanML3D/VQVAE-ParCo-t2m-default/00000-Trans-ParCo-default \
       --select-ckpt fid \
       --infer-mode userinput \
       --input-text 'an idol trainee is dancing like a basketball dribbling.' \
       --skip-path-check
       ```
     - æµ‹è¯•é›†ä½œä¸ºè¾“å…¥:
       ```
       CUDA_VISIBLE_DEVICES=0 python visualize/infer_motion_npy.py \
       --eval-exp-dir output/ParCo_official_HumanML3D/VQVAE-ParCo-t2m-default/00000-Trans-ParCo-default \
       --select-ckpt fid \
       --infer-mode testset \
       --skip-path-check
       ```
  1. æ¸²æŸ“`.npy`åŠ¨ä½œæ–‡ä»¶:
  
     è®°å¾—å®‰è£…æ¸²æŸ“æ‰€éœ€çš„åŒ…ï¼ˆåœ¨2.1. ç¯å¢ƒä¸­ï¼‰ã€‚
     å°†`--filedir`è®¾ç½®ä¸ºæ‚¨è‡ªå·±çš„åŠ¨ä½œæ–‡ä»¶ç›®å½•ã€‚
     
     æ¸²æŸ“çš„ç»“æœå°†ä¿å­˜åœ¨ä¸æ‚¨çš„åŠ¨ä½œæ–‡ä»¶ç›¸åŒçš„ç›®å½•ä¸‹çš„`rendered_motion`æ–‡ä»¶å¤¹ä¸­ã€‚
     ä¾‹å¦‚:
  
     ```
     CUDA_VISIBLE_DEVICES=0 python visualize/render_final.py --filedir output/visualize/00000-userinput/motion.npy 
     ```
     æ‚¨å°†å¾—åˆ°ä¸€ä¸ªæ¸²æŸ“çš„`.gif`åŠ¨ä½œã€‚ä¾‹å¦‚:
     <p align="center">
       <img src="../docs/imgs/rendered_motion_example.gif" width="20%" />
     </p>
  

- é€‰é¡¹2ï¼ˆæ¨èï¼‰ï¼šå‚è€ƒ[MLD](https://github.com/chenfengye/motion-latent-diffusion?tab=readme-ov-file).


## TODO
- [x] æ·»åŠ Demo
- [x] æ¸²æŸ“åŠ¨ä½œçš„å¯è§†åŒ–æ•™ç¨‹
- [x] README_zh.md


## è‡´è°¢

æˆ‘ä»¬æ„Ÿè°¢ï¼š
- å…¬å¼€ä»£ç : 
[T2M-GPT](https://github.com/Mael-zys/T2M-GPT), 
[MLD](https://github.com/chenfengye/motion-latent-diffusion?tab=readme-ov-file),
[MotionDiffuse](https://github.com/mingyuan-zhang/MotionDiffuse), 
[T2M](https://github.com/EricGuo5513/text-to-motion),
[TM2T](https://github.com/EricGuo5513/TM2T), 
[MDM](https://github.com/GuyTevet/motion-diffusion-model),
ç­‰ã€‚
- å…¬å¼€æ•°æ®é›†:[HumanML3D](https://github.com/EricGuo5513/HumanML3D)å’Œ[KIT-ML](https://arxiv.org/pdf/1607.03827.pdf)ã€‚

æ¥è‡ªæ–‡æœ¬åˆ°åŠ¨ä½œç¤¾åŒºçš„å…¶ä»–ä¼˜ç§€å…¬å¼€ä»£ç :
[ReMoDiffuse](https://github.com/mingyuan-zhang/ReMoDiffuse), [AttT2M](https://github.com/ZcyMonkey/AttT2M)ç­‰ã€‚



